# üï∏Ô∏è MCP Web Scraper Pro

**Serveur MCP professionnel pour web scraping intelligent : r√©cup√©ration, nettoyage, structuration et stockage automatique de contenu web**

Un Model Context Protocol (MCP) server complet qui permet √† Claude Code de scraper, parser, nettoyer et analyser n'importe quel site web de mani√®re √©thique et efficace.

---

## üéØ Fonctionnalit√©s principales

### ‚ú® Scraping intelligent
- **Mode HTML** : R√©cup√©ration du HTML brut nettoy√©
- **Mode Text** : Extraction du texte pur (sans balises)
- **Mode Structured** : JSON structur√© avec title, headings, paragraphes, links, images, meta

### üßπ Nettoyage automatique
- Suppression scripts/styles/trackers/publicit√©s
- √âlimination du "boilerplate" (header, footer, sidebar)
- Extraction du contenu principal
- D√©tection et suppression de spam

### üîç Extraction structur√©e
- Titre et m√©tadonn√©es (Open Graph, Twitter Cards)
- Headings (H1, H2, H3) hi√©rarchis√©s
- Paragraphes et sections
- Liens internes/externes avec contexte
- Images avec m√©tadonn√©es (alt, title, dimensions)
- D√©tection automatique du type de page (article, produit, doc, homepage)

### ü§ñ Crawler multi-pages
- Crawl limit√© et contr√¥l√© (max pages, same domain)
- Respect strict du robots.txt
- Rate limiting automatique + Crawl-delay
- Ignore patterns configurables
- Retry avec exponential backoff

### üíæ Stockage local (SQLite)
- Base de donn√©es SQLite int√©gr√©e
- Recherche full-text dans le contenu
- M√©tadonn√©es enrichies
- D√©duplication par URL

### üõ°Ô∏è S√©curit√© & √âthique
- Validation stricte des URLs (anti-SSRF)
- Respect obligatoire du robots.txt
- Rate limiting par domaine
- Blocage des IPs priv√©es
- User-Agent identifiable

---

## üìö Architecture

```
mcp-web-scraper-pro/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ index.ts                 # Point d'entr√©e
‚îÇ   ‚îú‚îÄ‚îÄ server.ts                # Serveur MCP + routing tools
‚îÇ   ‚îú‚îÄ‚îÄ types/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.ts           # Types et schemas Zod
‚îÇ   ‚îî‚îÄ‚îÄ scraper/
‚îÇ       ‚îú‚îÄ‚îÄ http-client.ts       # Client HTTP (axios, retries, robots.txt)
‚îÇ       ‚îú‚îÄ‚îÄ parser.ts            # Parsing HTML (Cheerio, Turndown)
‚îÇ       ‚îú‚îÄ‚îÄ cleaner.ts           # Nettoyage HTML
‚îÇ       ‚îú‚îÄ‚îÄ crawler.ts           # Crawler multi-pages
‚îÇ       ‚îî‚îÄ‚îÄ storage.ts           # Stockage SQLite
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ tsconfig.json
‚îú‚îÄ‚îÄ install.sh
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Pr√©requis

- **Node.js** >= 18.0.0
- **npm** >= 9.0.0

---

## üöÄ Installation

```bash
git clone <repo>/mcp-web-scraper-pro.git
cd mcp-web-scraper-pro
./install.sh
```

Le script installe automatiquement les d√©pendances, build le projet et configure l'environnement.

---

## üîß Configuration Claude Code

Fichier : `~/Library/Application Support/Claude/claude_desktop_config.json`

```json
{
  "mcpServers": {
    "web-scraper-pro": {
      "command": "node",
      "args": ["/home/user/mcp-web-scraper-pro/build/index.js"]
    }
  }
}
```

Red√©marrez Claude Code et v√©rifiez : `claude mcp list`

---

## üìñ Utilisation

### Exemples de commandes Claude Code

#### Scraper une page

```
Scrape cette page https://example.com et extrais son contenu structur√©
```

Claude appellera : `scrape_url` avec mode `structured`

#### Crawl un site

```
Crawl le site https://docs.example.com sur 20 pages maximum et stocke tout
```

Claude appellera :
1. `crawl` avec maxPages=20
2. `store_scraped_data` pour chaque page

#### Nettoyer du HTML

```
J'ai du HTML avec plein de scripts et pubs, nettoie-le :
<html>...</html>
```

Claude appellera : `clean_html`

#### Rechercher dans les donn√©es stock√©es

```
Cherche "machine learning" dans toutes les pages que j'ai scrapp√©es
```

Claude appellera : `search_stored_pages` avec query="machine learning"

---

## üõ†Ô∏è Tools MCP disponibles

| Tool | Description |
|------|-------------|
| `scrape_url` | Scrape une URL (modes: html, text, structured) |
| `clean_html` | Nettoie HTML (supprime scripts/styles/trackers) |
| `extract_structured` | Extrait contenu structur√© d'un HTML |
| `list_links` | Liste tous les liens (internes/externes) |
| `crawl` | Crawl multi-pages avec limites |
| `store_scraped_data` | Stocke une page en SQLite |
| `get_stored_page` | R√©cup√®re page par URL ou ID |
| `delete_stored_page` | Supprime page stock√©e |
| `search_stored_pages` | Recherche full-text |
| `validate_url` | Valide URL (robots.txt, HTTPS) |
| `scraper_status` | Stats du scraper |

**Total : 11 tools**

---

## üîí S√©curit√©

### Protections impl√©ment√©es

‚úÖ **Anti-SSRF** : Blocage des IPs priv√©es (127.0.0.1, 10.x.x.x, 192.168.x.x, localhost)
‚úÖ **Robots.txt** : Respect strict (v√©rification automatique avant chaque scrape)
‚úÖ **Rate limiting** : Minimum 1 seconde entre requ√™tes sur m√™me domaine
‚úÖ **Crawl-delay** : D√©tection et respect du Crawl-delay d√©fini dans robots.txt
‚úÖ **Timeout** : Limite de 10 secondes par requ√™te (configurable)
‚úÖ **Max pages** : Limit√© √† 100 pages par crawl
‚úÖ **Retry logic** : Exponential backoff (1s, 2s, 4s) avec max 3 tentatives
‚úÖ **User-Agent** : Identifiable (`MCPWebScraper/1.0`)

### URLs interdites

- IPs priv√©es : `127.0.0.1`, `10.x.x.x`, `172.16-31.x.x`, `192.168.x.x`
- Localhost : `localhost`, `0.0.0.0`
- Protocoles non-HTTP : `file://`, `ftp://`, etc.

---

## üêõ D√©pannage

### Probl√®me : "Robots.txt interdit l'acc√®s"

**Solution** : Normal, le site bloque les scrapers. Vous pouvez :
- Respecter la d√©cision du site
- Utiliser `respectRobotsTxt: false` (d√©conseill√© √©thiquement)

### Probl√®me : "Erreur HTTP 403 Forbidden"

**Solution** :
- Le site d√©tecte le User-Agent du scraper
- Certains sites bloquent les scrapers
- Respectez la politique du site

### Probl√®me : "Timeout apr√®s 10 secondes"

**Solution** :
- Augmenter le timeout : `scrape_url` avec `timeout: 30000`
- V√©rifier votre connexion internet

### Probl√®me : Base de donn√©es SQLite verrouill√©e

**Solution** :
```bash
rm scraped_data.db
# Relancer le serveur
```

---

## üîß Extension

### Ajouter un nouveau pattern de nettoyage

Modifiez `src/scraper/cleaner.ts` :

```typescript
private removeCustomElements($: cheerio.CheerioAPI): void {
  $('.my-custom-class, #my-custom-id').remove();
}
```

### Ajouter un nouveau format d'export

Modifiez `src/scraper/storage.ts` pour supporter JSON, CSV, Markdown, etc.

---

## üìä Cas d'usage

### 1. Veille concurrentielle
Crawl les blogs/sites concurrents et analyse automatique des nouveaux articles.

### 2. Documentation technique
Scrape des docs techniques (API, frameworks) pour analyse par l'IA.

### 3. R√©sum√© automatique
Scrape un article ‚Üí Claude r√©sume ‚Üí Stockage du r√©sum√©.

### 4. Data mining
Extraction de donn√©es structur√©es depuis des sites (prix, produits, articles).

### 5. Monitoring de changements
Scrape r√©gulier + comparaison pour d√©tecter modifications.

---

## üöÄ Roadmap V2

### Court terme
- [ ] Support Playwright pour pages JS-heavy
- [ ] Export CSV/JSON/Markdown
- [ ] Webhooks pour notifications
- [ ] Support pagination automatique

### Moyen terme
- [ ] Extraction s√©mantique (embeddings)
- [ ] Classification automatique du contenu
- [ ] D√©tection de langue
- [ ] Support multi-format (PDF, DOCX via conversion)

### Long terme
- [ ] Crawler distribu√© (Redis queue)
- [ ] Cache intelligent (√©viter re-scrape)
- [ ] API REST en plus de MCP
- [ ] Dashboard web de monitoring

---

## üìù Licence

MIT

---

## ü§ù Contribution

Les contributions sont bienvenues ! Ouvrez une issue ou une PR.

---

## üìö Sources

- [Model Context Protocol](https://modelcontextprotocol.io/)
- [Claude Code MCP Docs](https://code.claude.com/docs/en/mcp)
- [Cheerio Documentation](https://cheerio.js.org/)
- [Robots.txt Spec](https://www.robotstxt.org/)

---

**D√©velopp√© avec ‚ù§Ô∏è pour Claude Code**
