# ğŸ§  Local LLM MCP - Assistant IA Local

**Serveur MCP pour interagir avec des modÃ¨les IA locaux**

Support pour :
- ğŸ¦™ **Ollama** - ModÃ¨les locaux optimisÃ©s
- ğŸ¨ **LM Studio** - Interface OpenAI-compatible
- ğŸ¤– **GPT4All** - ModÃ¨les locaux lÃ©gers
- ğŸŒ **Qwen** - ModÃ¨les Qwen en mode serveur

---

## ğŸ¯ FonctionnalitÃ©s

âœ… **6 tools MCP** pour interagir avec des LLM locaux
âœ… **Multi-backend** - Support Ollama, LM Studio, GPT4All, Qwen
âœ… **DÃ©tection automatique** - VÃ©rifie les backends disponibles
âœ… **Fallback intelligent** - Bascule automatique si un backend Ã©choue
âœ… **ParamÃ¨tres ajustables** - Temperature, top_p, max_tokens
âœ… **Mode chat** - Conversations multi-tours
âœ… **SÃ©curisÃ©** - Sandbox texte uniquement, pas d'exÃ©cution shell

---

## ğŸ“‹ Tools disponibles

| Tool | Description |
|------|-------------|
| `llm_list_models` | Liste tous les modÃ¨les disponibles |
| `llm_run_inference` | ExÃ©cute une gÃ©nÃ©ration de texte |
| `llm_chat` | Conversation avec le modÃ¨le |
| `llm_model_info` | Informations sur un modÃ¨le |
| `llm_set_backend` | Change le backend (ollama/lmstudio) |
| `llm_get_backend` | Retourne le backend courant |

---

## ğŸ”§ PrÃ©requis

### Obligatoires
- **Node.js** >= 18.0.0
- **npm** >= 9.0.0

### Au moins un backend installÃ©
- **Ollama** : https://ollama.ai/download
- **LM Studio** : https://lmstudio.ai/
- **GPT4All** : https://gpt4all.io/
- **Qwen** : Mode serveur local

---

## ğŸ“¦ Installation

```bash
# Cloner/tÃ©lÃ©charger le projet
cd local_llm_mcp

# Installer les dÃ©pendances
npm install

# Compiler TypeScript
npm run build

# Configurer l'environnement
cp .env.example .env
nano .env  # Ã‰diter les URLs des backends
```

### Configuration `.env`

```bash
MCP_PORT=3200
OLLAMA_URL=http://localhost:11434
LMSTUDIO_URL=http://localhost:1234
DEFAULT_BACKEND=ollama
```

---

## ğŸš€ Utilisation

### DÃ©marrer le serveur

```bash
# Mode production
npm start

# Mode dÃ©veloppement
npm run dev
```

### Health check

```bash
curl http://localhost:3200/health
```

### Connexion Ã  Claude Code CLI

```bash
# MÃ©thode stdio (recommandÃ©)
claude mcp add llm-assistant stdio node /path/to/local_llm_mcp/dist/server.js

# MÃ©thode HTTP
npm start  # Dans un terminal
claude mcp add llm-assistant http://localhost:3200  # Dans un autre
```

---

## ğŸ’¡ Exemples d'utilisation

### Lister les modÃ¨les

```json
{
  "name": "llm_list_models",
  "arguments": {
    "backend": "ollama"
  }
}
```

**RÃ©sultat** :
```json
{
  "success": true,
  "data": [
    { "name": "llama2:7b", "size": 3826793000 },
    { "name": "mistral:latest", "size": 4109860000 }
  ]
}
```

### GÃ©nÃ©ration de texte

```json
{
  "name": "llm_run_inference",
  "arguments": {
    "model": "llama2:7b",
    "prompt": "Explique moi la diffÃ©rence entre Docker et une VM",
    "temperature": 0.7,
    "max_tokens": 500
  }
}
```

### Conversation

```json
{
  "name": "llm_chat",
  "arguments": {
    "model": "llama2:7b",
    "messages": [
      { "role": "system", "content": "Tu es un assistant technique expert." },
      { "role": "user", "content": "Comment crÃ©er un serveur Express.js ?" }
    ],
    "temperature": 0.8
  }
}
```

### Changer de backend

```json
{
  "name": "llm_set_backend",
  "arguments": {
    "backend": "lmstudio"
  }
}
```

---

## ğŸ”„ Workflows typiques

### Workflow 1 : PremiÃ¨re utilisation

```bash
1. llm_get_backend â†’ VÃ©rifier le backend courant
2. llm_list_models â†’ Lister les modÃ¨les disponibles
3. llm_run_inference â†’ Tester avec un modÃ¨le
```

### Workflow 2 : Conversation multi-tours

```bash
1. llm_chat â†’ Premier message
2. llm_chat â†’ Continuer la conversation (avec historique)
3. llm_chat â†’ Question de suivi
```

### Workflow 3 : Fallback

```bash
1. llm_set_backend â†’ "ollama"
2. llm_run_inference â†’ Si Ã©choue (Ollama offline)
3. llm_set_backend â†’ "lmstudio"
4. llm_run_inference â†’ Retry avec LM Studio
```

---

## ğŸ› DÃ©pannage

### Ollama ne rÃ©pond pas

**ProblÃ¨me** : `Connection refused`

**Solutions** :
```bash
# VÃ©rifier qu'Ollama tourne
ollama list

# DÃ©marrer Ollama
ollama serve

# Tester l'API
curl http://localhost:11434/api/tags
```

### LM Studio ne rÃ©pond pas

**ProblÃ¨me** : `ECONNREFUSED`

**Solutions** :
1. Ouvrir LM Studio
2. Aller dans "Local Server"
3. Cliquer sur "Start Server"
4. VÃ©rifier le port (dÃ©faut: 1234)

### ModÃ¨le introuvable

**ProblÃ¨me** : `model not found`

**Solutions** :
```bash
# Pour Ollama
ollama pull llama2:7b

# Pour LM Studio
# TÃ©lÃ©charger le modÃ¨le via l'interface
```

---

## ğŸ”’ SÃ©curitÃ©

âœ… **Sandbox texte** - Pas d'accÃ¨s fichiers
âœ… **Pas d'exÃ©cution shell** - Aucune commande systÃ¨me
âœ… **Limite de taille** - Prompts limitÃ©s Ã  50KB
âœ… **Timeout** - 120s par dÃ©faut
âœ… **Validation inputs** - Schemas stricts

**SÃ»r pour** :
- GÃ©nÃ©ration de texte
- Conversations
- Assistance au code
- RÃ©sumÃ©s

**Non prÃ©vu pour** :
- ExÃ©cution de code
- AccÃ¨s fichiers
- OpÃ©rations systÃ¨me

---

## ğŸš€ Roadmap & AmÃ©liorations

### Version 1.1 (Court terme)
- [ ] Support GPT4All complet
- [ ] Support Qwen local
- [ ] Streaming SSE pour rÃ©ponses
- [ ] Cache des rÃ©ponses
- [ ] Historique des conversations

### Version 1.2 (Moyen terme)
- [ ] Multi-backend simultanÃ©
- [ ] Load balancing automatique
- [ ] Embeddings locaux
- [ ] RAG local avec vector DB
- [ ] Fine-tuning local

### Version 2.0 (Long terme)
- [ ] Interface graphique web
- [ ] API REST complÃ¨te
- [ ] SystÃ¨me de plugins
- [ ] ModÃ¨les spÃ©cialisÃ©s (code, math, etc.)
- [ ] AccÃ©lÃ©ration GPU avancÃ©e

---

## ğŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CLAUDE CODE CLI                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ MCP Protocol
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    LOCAL LLM MCP SERVER (Port 3200)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Backend Manager | Tools Registry         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚       â”‚       â”‚       â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”
    â–¼      â–¼       â–¼       â–¼       â–¼
 Ollama  LMStudio  Qwen  GPT4All  [Autres]
:11434   :1234   :8000  :4891
```

---

## ğŸ“ Licence

MIT License

---

## ğŸ™ Remerciements

- [Ollama](https://ollama.ai) - ModÃ¨les locaux optimisÃ©s
- [LM Studio](https://lmstudio.ai) - Interface conviviale
- [GPT4All](https://gpt4all.io) - ModÃ¨les open-source
- [Anthropic](https://anthropic.com) - Model Context Protocol

---

**CrÃ©Ã© avec â¤ï¸ par Skynet AI Assistant**
**Version** : 1.0.0
