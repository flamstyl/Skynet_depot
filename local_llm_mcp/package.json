{
  "name": "local-llm-mcp",
  "version": "1.0.0",
  "description": "MCP Server pour LLM locaux (Ollama, LM Studio, GPT4All, Qwen)",
  "type": "module",
  "main": "dist/server.js",
  "scripts": {
    "build": "tsc",
    "dev": "tsx watch src/server.ts",
    "start": "node dist/server.js",
    "test": "jest"
  },
  "keywords": ["mcp", "llm", "ollama", "lm-studio", "gpt4all", "local-ai"],
  "author": "Skynet AI Assistant",
  "license": "MIT",
  "dependencies": {
    "express": "^4.18.2",
    "axios": "^1.6.2",
    "dotenv": "^16.3.1",
    "winston": "^3.11.0",
    "joi": "^17.11.0"
  },
  "devDependencies": {
    "@types/express": "^4.17.21",
    "@types/node": "^20.10.5",
    "typescript": "^5.3.3",
    "tsx": "^4.7.0",
    "jest": "^29.7.0"
  }
}
