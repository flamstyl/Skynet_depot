# üü£ MCP_Browser_LMStudio

**Serveur MCP local** permettant √† une IA (Claude, GPT, Gemini) de contr√¥ler un navigateur web et d'interagir avec **LM Studio** (mod√®le IA local).

## üéØ Fonctionnalit√©s

- ‚úÖ **Serveur FastAPI** avec API REST compl√®te
- ‚úÖ **Navigateur contr√¥lable** (Playwright headless ou PyWebView)
- ‚úÖ **Client LM Studio** pour interroger des mod√®les locaux
- ‚úÖ **M√©moire des interactions** (historique navigateur)
- ‚úÖ **Authentification par token**
- ‚úÖ **Logs d√©taill√©s** (`logs/server.log`)
- ‚úÖ **Screenshots** avec export base64

## üìÅ Structure du Projet

```
MCP_Browser_LMStudio/
‚îú‚îÄ‚îÄ app.py                      # Serveur FastAPI principal
‚îú‚îÄ‚îÄ browser_controller.py       # Contr√¥leur navigateur (Playwright/PyWebView)
‚îú‚îÄ‚îÄ lmstudio_client.py         # Client LM Studio API
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ browser_models.py      # Schemas Pydantic navigateur
‚îÇ   ‚îî‚îÄ‚îÄ lm_models.py           # Schemas Pydantic LM Studio
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ server.log             # Logs du serveur
‚îú‚îÄ‚îÄ config.json                # Configuration
‚îú‚îÄ‚îÄ requirements.txt           # D√©pendances Python
‚îî‚îÄ‚îÄ README.md                  # Ce fichier
```

## üöÄ Installation

### Pr√©requis

- **Python 3.11+**
- **LM Studio** install√© et lanc√© (https://lmstudio.ai)
- Un mod√®le charg√© dans LM Studio

### 1. Installer les d√©pendances

```bash
cd MCP_Browser_LMStudio
pip install -r requirements.txt
```

### 2. Installer Playwright Chromium

```bash
playwright install chromium
```

### 3. Configurer `config.json`

√âditez `config.json` :

```json
{
  "auth_token": "VOTRE_TOKEN_SECRET_ICI",
  "browser_engine": "playwright",
  "lmstudio": {
    "host": "http://localhost:1234",
    "model": null
  },
  "server": {
    "host": "0.0.0.0",
    "port": 8000
  }
}
```

**Important** : Changez `auth_token` pour s√©curiser votre serveur.

### 4. Lancer LM Studio

1. Ouvrez **LM Studio**
2. Chargez un mod√®le (ex: `granite-3.0-2b-instruct`)
3. Allez dans l'onglet **Developer**
4. Cliquez sur **Start Server** (port 1234 par d√©faut)

### 5. Lancer le serveur MCP

```bash
python app.py
```

Vous devriez voir :

```
üöÄ D√©marrage de MCP_Browser_LMStudio
‚úÖ LM Studio est accessible: ...
‚úÖ Serveur pr√™t
INFO:     Uvicorn running on http://0.0.0.0:8000
```

## üß™ Tests des Routes

### 1. V√©rifier le statut global

```bash
curl http://localhost:8000/status \
  -H "Authorization: Bearer VOTRE_TOKEN_SECRET_ICI"
```

**R√©ponse attendue** :

```json
{
  "server": "MCP_Browser_LMStudio",
  "version": "1.0.0",
  "status": "running",
  "browser": {
    "running": false,
    "engine": "playwright",
    "current_url": null,
    "title": null
  },
  "lmstudio": {
    "available": true,
    "host": "http://localhost:1234",
    "message": "LM Studio est accessible. 1 mod√®le(s) disponible(s).",
    "models_loaded": 1
  }
}
```

### 2. Ouvrir une page web

```bash
curl -X POST http://localhost:8000/browser/open \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer VOTRE_TOKEN_SECRET_ICI" \
  -d '{
    "url": "https://example.com",
    "wait_time": 2.0
  }'
```

**R√©ponse** :

```json
{
  "url": "https://example.com/",
  "title": "Example Domain",
  "html_preview": "<!doctype html>\n<html>\n<head>\n    <title>Example Domain</title>...",
  "timestamp": "2025-01-22T10:30:00",
  "success": true
}
```

### 3. R√©cup√©rer le HTML de la page

```bash
curl http://localhost:8000/browser/get_html \
  -H "Authorization: Bearer VOTRE_TOKEN_SECRET_ICI"
```

### 4. Cliquer sur un √©l√©ment

```bash
curl -X POST http://localhost:8000/browser/click \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer VOTRE_TOKEN_SECRET_ICI" \
  -d '{
    "selector": "#login-button",
    "wait_after": 1.0
  }'
```

### 5. Prendre une capture d'√©cran

```bash
curl -X POST http://localhost:8000/browser/screenshot \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer VOTRE_TOKEN_SECRET_ICI" \
  -d '{
    "full_page": false
  }'
```

**R√©ponse** :

```json
{
  "filename": "screenshot_20250122_103000.png",
  "filepath": "logs/screenshot_20250122_103000.png",
  "base64_image": "iVBORw0KGgoAAAANSUhEUgAA...",
  "timestamp": "2025-01-22T10:30:00",
  "success": true
}
```

### 6. Interroger LM Studio

```bash
curl -X POST http://localhost:8000/lm/query \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer VOTRE_TOKEN_SECRET_ICI" \
  -d '{
    "prompt": "Explique-moi ce qu'\''est le Model Context Protocol (MCP)",
    "temperature": 0.7,
    "max_tokens": 500
  }'
```

**R√©ponse** :

```json
{
  "response": "Le Model Context Protocol (MCP) est un protocole standardis√©...",
  "model": "granite-3.0-2b-instruct",
  "tokens_used": 245,
  "temperature": 0.7,
  "timestamp": "2025-01-22T10:30:00",
  "success": true
}
```

### 7. Liste des mod√®les LM Studio

```bash
curl http://localhost:8000/lm/models \
  -H "Authorization: Bearer VOTRE_TOKEN_SECRET_ICI"
```

### 8. Historique des interactions

```bash
curl http://localhost:8000/memory/history \
  -H "Authorization: Bearer VOTRE_TOKEN_SECRET_ICI"
```

**R√©ponse** :

```json
{
  "total_interactions": 3,
  "interactions": [
    {
      "timestamp": "2025-01-22T10:25:00",
      "action": "open",
      "details": {"url": "https://example.com"},
      "success": true
    },
    {
      "timestamp": "2025-01-22T10:26:00",
      "action": "screenshot",
      "details": {"filename": "screenshot_20250122_102600.png"},
      "success": true
    },
    {
      "timestamp": "2025-01-22T10:27:00",
      "action": "get_html",
      "details": {"url": "https://example.com/"},
      "success": true
    }
  ]
}
```

## üîê S√©curit√©

### Authentification

Toutes les routes n√©cessitent un header `Authorization` :

```
Authorization: Bearer VOTRE_TOKEN_SECRET_ICI
```

‚ö†Ô∏è **Important** : Changez le token par d√©faut dans `config.json` !

### Logs

Tous les √©v√©nements sont enregistr√©s dans `logs/server.log` :

```
2025-01-22 10:25:00 - INFO - üìñ Ouverture de l'URL: https://example.com
2025-01-22 10:26:00 - INFO - üì∏ Capture d'√©cran: auto
2025-01-22 10:27:00 - INFO - ü§ñ Requ√™te LM Studio: Explique-moi...
```

## üîß Configuration Avanc√©e

### Choisir le moteur de navigateur

Dans `config.json` :

```json
{
  "browser_engine": "playwright"  // ou "pywebview"
}
```

**Recommand√©** : `playwright` (plus stable, plus de fonctionnalit√©s)

### Configuration LM Studio

Si LM Studio utilise un port diff√©rent :

```json
{
  "lmstudio": {
    "host": "http://localhost:5000",
    "model": "granite-3.0-2b-instruct"
  }
}
```

## ü§ù Int√©gration avec Claude / GPT / Gemini

### Avec Claude Desktop (MCP)

Ajoutez dans `claude_desktop_config.json` :

```json
{
  "mcpServers": {
    "mcp_browser_lmstudio": {
      "command": "python",
      "args": ["/chemin/vers/MCP_Browser_LMStudio/app.py"]
    }
  }
}
```

### Avec Claude Code / API

Claude peut appeler directement les endpoints HTTP :

```python
# Exemple d'utilisation depuis Claude Code
import httpx

async with httpx.AsyncClient() as client:
    # Ouvrir une page
    response = await client.post(
        "http://localhost:8000/browser/open",
        json={"url": "https://example.com"},
        headers={"Authorization": "Bearer VOTRE_TOKEN"}
    )

    # R√©cup√©rer le HTML
    html_response = await client.get(
        "http://localhost:8000/browser/get_html",
        headers={"Authorization": "Bearer VOTRE_TOKEN"}
    )

    # Interroger LM Studio
    lm_response = await client.post(
        "http://localhost:8000/lm/query",
        json={"prompt": "Analyse cette page: " + html_response.json()["html"]},
        headers={"Authorization": "Bearer VOTRE_TOKEN"}
    )
```

## üìö Documentation API

Acc√©dez √† la documentation interactive Swagger :

```
http://localhost:8000/docs
```

Ou ReDoc :

```
http://localhost:8000/redoc
```

## üêõ D√©pannage

### LM Studio n'est pas accessible

**Erreur** : `LM Studio n'est pas accessible`

**Solutions** :
1. V√©rifiez que LM Studio est lanc√©
2. Allez dans l'onglet **Developer** ‚Üí **Start Server**
3. V√©rifiez le port dans `config.json` (par d√©faut: 1234)

### Playwright ne fonctionne pas

**Erreur** : `Playwright is not installed`

**Solution** :

```bash
playwright install chromium
```

### Erreur d'authentification

**Erreur** : `401 Unauthorized`

**Solution** : V√©rifiez que vous utilisez le bon token :

```bash
curl ... -H "Authorization: Bearer VOTRE_TOKEN_SECRET_ICI"
```

### Port d√©j√† utilis√©

**Erreur** : `Address already in use`

**Solution** : Changez le port dans `config.json` :

```json
{
  "server": {
    "port": 8001
  }
}
```

## üîÑ Workflow Recommand√©

1. **D√©marrer LM Studio** et charger un mod√®le
2. **Lancer le serveur MCP** : `python app.py`
3. **V√©rifier le statut** : `curl http://localhost:8000/status`
4. **Ouvrir une page** via `/browser/open`
5. **R√©cup√©rer le HTML** via `/browser/get_html`
6. **Interroger LM Studio** avec le HTML : `/lm/query`
7. **Consulter l'historique** : `/memory/history`

## üìù Notes de D√©veloppement

### Architecture

- **FastAPI** : Serveur HTTP asynchrone
- **Playwright** : Contr√¥le du navigateur (recommand√©)
- **PyWebView** : Alternative l√©g√®re (limit√©e)
- **LM Studio Client** : Bas√© sur la doc officielle (`/api/v0/chat/completions`)
- **Pydantic** : Validation des donn√©es

### Endpoints LM Studio

Conform√©ment √† la [documentation officielle](https://lmstudio.ai/docs/developer/rest/endpoints) :

- ‚úÖ `POST /api/v0/chat/completions` (utilis√©)
- ‚úÖ `GET /api/v0/models` (utilis√©)
- ‚ö†Ô∏è `/v1/*` endpoints (compatibilit√© OpenAI) non utilis√©s

## üìÑ Licence

Ce projet fait partie de l'√©cosyst√®me **Skynet**.

## üôè Cr√©dits

- **LM Studio** : https://lmstudio.ai
- **FastAPI** : https://fastapi.tiangolo.com
- **Playwright** : https://playwright.dev
- **Model Context Protocol (MCP)** : Anthropic

---

**D√©velopp√© avec üü£ pour l'√©cosyst√®me Skynet**
