# üöÄ Guide Complet des MCP Servers Skynet

Ce document regroupe les deux MCP servers d√©velopp√©s pour transformer Claude Code en v√©ritable poste de travail DevOps/Cr√©atif + Web Scraping.

---

## üì¶ Vue d'ensemble des projets

### 1Ô∏è‚É£ MCP DevOps Workspace
**50+ tools pour d√©veloppement, Docker, syst√®me, projets, graphisme**

### 2Ô∏è‚É£ MCP Web Scraper Pro
**11 tools pour scraping, parsing, nettoyage, crawling, stockage**

---

## üèóÔ∏è Architecture globale

Les deux MCP servers suivent la m√™me architecture :

```
TypeScript + Node.js >= 18
     ‚Üì
@modelcontextprotocol/sdk (officiel)
     ‚Üì
Tools MCP expos√©s via JSON-RPC
     ‚Üì
Claude Code (Client MCP)
```

### Stack technique commune

- **Langage** : TypeScript (type-safety + SDK officiel mature)
- **Runtime** : Node.js >= 18
- **Validation** : Zod (schemas JSON Schema)
- **Protocole** : MCP via stdio (JSON-RPC)
- **D√©ploiement** : npx (zero-config)

---

## üìä Comparaison des deux MCP

| Aspect | DevOps Workspace | Web Scraper Pro |
|--------|------------------|-----------------|
| **Nombre de tools** | 50+ | 11 |
| **Domaines** | 5 (dev, docker, syst√®me, projets, graphisme) | 1 (web scraping) |
| **D√©pendances principales** | dockerode, systeminformation, sharp, simple-git | cheerio, axios, better-sqlite3, turndown |
| **Stockage** | Aucun (actions en temps r√©el) | SQLite local |
| **S√©curit√©** | Path traversal, confirmation actions | Anti-SSRF, robots.txt, rate limiting |
| **Cas d'usage** | Admin serveur, dev env, gestion projets | Veille, data mining, doc scraping |

---

## üöÄ Installation rapide (les deux)

```bash
cd /home/user/Skynet_depot

# MCP DevOps Workspace
cd mcp-devops-workspace
./install.sh

# MCP Web Scraper Pro
cd ../mcp-web-scraper-pro
./install.sh
```

### Configuration Claude Code (les deux en parall√®le)

`~/Library/Application Support/Claude/claude_desktop_config.json` :

```json
{
  "mcpServers": {
    "devops-workspace": {
      "command": "node",
      "args": ["/home/user/Skynet_depot/mcp-devops-workspace/build/index.js"]
    },
    "web-scraper-pro": {
      "command": "node",
      "args": ["/home/user/Skynet_depot/mcp-web-scraper-pro/build/index.js"]
    }
  }
}
```

Red√©marrez Claude Code ‚Üí `claude mcp list` ‚Üí Devrait afficher les 2 MCP

---

## üí° Cas d'usage combin√©s (les deux MCP ensemble)

### üî• Sc√©nario 1 : Veille concurrentielle automatis√©e

```
1. Web Scraper Pro ‚Üí Crawl le blog concurrent
2. DevOps Workspace ‚Üí Stocke le r√©sum√© dans un projet Git
3. DevOps Workspace ‚Üí Commit et push automatique
```

**Commande Claude** :
```
Scrape les 10 derniers articles de https://blog.concurrent.com,
r√©sume-les et cr√©e un fichier veille-concurrent.md dans mon projet,
puis commit et push sur GitHub
```

### üî• Sc√©nario 2 : Documentation technique automatique

```
1. Web Scraper Pro ‚Üí Scrape docs API (ex: docs.stripe.com)
2. DevOps Workspace ‚Üí Cr√©e un projet Python avec exemples
3. DevOps Workspace ‚Üí Configure virtualenv + install deps
```

**Commande Claude** :
```
Scrape la doc de l'API Stripe pour les paiements,
cr√©e un projet Python "stripe-integration",
g√©n√®re des exemples de code et installe les d√©pendances
```

### üî• Sc√©nario 3 : Monitoring & health check

```
1. DevOps Workspace ‚Üí Check ressources syst√®me (CPU, RAM, disque)
2. DevOps Workspace ‚Üí Liste containers Docker + logs
3. Web Scraper Pro ‚Üí Scrape status page externe
4. DevOps Workspace ‚Üí Cr√©er rapport et commit
```

**Commande Claude** :
```
Fais-moi un rapport complet sur l'√©tat de mon serveur :
- ressources syst√®me
- √©tat des containers Docker
- scrape https://status.myservice.com pour v√©rifier uptime
- g√©n√®re un rapport Markdown et commit-le
```

---

## üéØ Workflows recommand√©s

### Workflow 1 : Setup nouveau projet full-stack

```bash
# Via DevOps Workspace
1. create_project (structure projet)
2. setup_python_env (backend)
3. setup_node_env (frontend)
4. git_init + git_commit
5. docker_compose_up (si stack Docker)
```

### Workflow 2 : Scraping + analyse + stockage

```bash
# Via Web Scraper Pro + DevOps Workspace
1. scrape_url (r√©cup√©rer contenu)
2. clean_html (nettoyer)
3. extract_structured (structurer)
4. store_scraped_data (SQLite)
5. write_file (exporter en Markdown via DevOps)
6. git_add + git_commit (version control)
```

### Workflow 3 : Monitoring quotidien

```bash
# Via DevOps Workspace
1. get_system_info
2. get_resource_usage
3. docker_stats
4. list_services
5. G√©n√©rer rapport automatique
```

---

## üîí S√©curit√© globale

### Protections communes

‚úÖ **Validation stricte** : Zod schemas pour tous les inputs
‚úÖ **Gestion d'erreurs** : Try/catch + formatError MCP
‚úÖ **Logs** : console.error pour debugging

### DevOps Workspace - S√©curit√© sp√©cifique

‚úÖ Path traversal protection
‚úÖ Paths prot√©g√©s (/etc/passwd, /root, etc.)
‚úÖ Confirmation pour actions dangereuses (delete_file, restart_service)
‚úÖ Backup automatique avant write_file
‚úÖ Taille max fichiers (10MB)

### Web Scraper Pro - S√©curit√© sp√©cifique

‚úÖ Anti-SSRF (blocage IPs priv√©es)
‚úÖ Respect robots.txt obligatoire
‚úÖ Rate limiting (1s minimum entre requ√™tes)
‚úÖ Timeout 10s par requ√™te
‚úÖ Max 100 pages par crawl
‚úÖ Retry avec exponential backoff

---

## üìö Documentation

- **DevOps Workspace** : `/mcp-devops-workspace/README.md`
- **Web Scraper Pro** : `/mcp-web-scraper-pro/README.md`

---

## üß™ Tests recommand√©s

### Tests DevOps Workspace

```bash
# Test create_project
node build/index.js
# Dans Claude : "Cr√©e un projet Python test-project"

# Test Docker (si Docker install√©)
# Dans Claude : "Liste mes containers Docker"

# Test Git
# Dans Claude : "Dans /tmp/test, init git et commit"
```

### Tests Web Scraper Pro

```bash
# Test scrape simple
# Dans Claude : "Scrape https://example.com en mode structur√©"

# Test crawl
# Dans Claude : "Crawl https://example.com sur 5 pages"

# Test stockage
# Dans Claude : "Stocke la page scrapp√©e et recherche 'example'"
```

---

## üêõ D√©pannage global

### MCP non d√©tect√© par Claude Code

**Solutions** :
1. V√©rifier le chemin dans `claude_desktop_config.json`
2. V√©rifier que `build/index.js` existe pour les deux projets
3. Rebuild : `npm run build` dans chaque projet
4. Red√©marrer Claude Code
5. Logs : chercher erreurs dans Console Claude Code

### Build TypeScript √©choue

**Solution** :
```bash
# Pour chaque projet
rm -rf node_modules build
npm install
npm run build
```

### Permissions insuffisantes

**DevOps Workspace** :
- Docker : `sudo usermod -aG docker $USER` puis logout/login
- systemd : certaines commandes n√©cessitent sudo

**Web Scraper Pro** :
- SQLite : v√©rifier permissions dossier `scraped_data/`

---

## üöÄ Roadmap globale

### Court terme (1-2 mois)

**DevOps Workspace** :
- [ ] Support Kubernetes (kubectl)
- [ ] Monitoring Prometheus/Grafana
- [ ] CI/CD GitHub Actions
- [ ] Support bases de donn√©es (PostgreSQL, MySQL, Redis)

**Web Scraper Pro** :
- [ ] Support Playwright (pages JavaScript)
- [ ] Export CSV/JSON/Markdown
- [ ] Webhooks pour notifications
- [ ] Pagination automatique

### Moyen terme (3-6 mois)

**DevOps Workspace** :
- [ ] Dashboard web de monitoring
- [ ] Int√©gration Terraform/Ansible
- [ ] Support multi-cloud (AWS, GCP, Azure via SDK)
- [ ] Alerting automatique (Slack, Discord, Email)

**Web Scraper Pro** :
- [ ] Extraction s√©mantique (embeddings)
- [ ] Classification automatique (ML)
- [ ] D√©tection de langue
- [ ] Support PDF/DOCX

### Long terme (6-12 mois)

**DevOps Workspace** :
- [ ] Multi-serveurs (SSH remote)
- [ ] Orchestration compl√®te (d√©ploiements)
- [ ] Int√©gration compl√®te CI/CD
- [ ] Backup/restore automatique

**Web Scraper Pro** :
- [ ] Crawler distribu√© (Redis queue)
- [ ] Cache intelligent
- [ ] API REST en plus de MCP
- [ ] Dashboard web monitoring

---

## ü§ù Contribution

Les deux projets sont open-source (MIT). Contributions bienvenues !

**Process** :
1. Fork le projet
2. Cr√©er branche : `git checkout -b feature/ma-feature`
3. Commit : `git commit -m "feat: ma feature"`
4. Push : `git push origin feature/ma-feature`
5. Ouvrir PR

---

## üìä Statistiques

**MCP DevOps Workspace** :
- 50+ tools
- 16 fichiers source
- ~3500 lignes de code TypeScript
- 5 modules (dev_env, docker_admin, server_admin, project_ops, graphics_tools)

**MCP Web Scraper Pro** :
- 11 tools
- 13 fichiers source
- ~1900 lignes de code TypeScript
- 5 modules scraper (http-client, parser, cleaner, crawler, storage)

**Total : 60+ tools MCP professionnels pour Claude Code**

---

## üéâ Remerciements

- **Anthropic** : Model Context Protocol + Claude
- **Communaut√© open-source** : Biblioth√®ques utilis√©es (dockerode, cheerio, sharp, etc.)

---

## üìù Licence

MIT

---

**D√©velopp√© avec ‚ù§Ô∏è pour Claude Code par Skynet Depot**
