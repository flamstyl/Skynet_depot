# ğŸ“ MCP Directory - Model Context Protocol Scripts

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

## ğŸ¯ Vue d'ensemble

Le rÃ©pertoire `/opt/mcp/` contient l'ensemble des **scripts d'orchestration et d'automatisation** pour l'environnement IA Ubuntu VM. Ces scripts permettent de gÃ©rer l'installation, la surveillance et le lancement d'agents IA comme Claude CLI, Ollama, Gemini CLI, etc.

## ğŸ“‹ Scripts disponibles

### ğŸ”§ `install.sh` - Installation des outils IA

**RÃ´le:** Installer tous les outils et dÃ©pendances nÃ©cessaires pour les agents IA.

**Contenu:**
- Mise Ã  jour du systÃ¨me Ubuntu
- Installation de paquets Python (anthropic, openai, google-generativeai)
- Installation de Claude CLI (wrapper ou CLI officiel)
- Installation d'Ollama (LLM local)
- Installation de Gemini CLI (wrapper Python)
- Installation d'outils graphiques (GIMP, Inkscape, ImageMagick, FFmpeg)
- Installation de Docker CLI
- Configuration de l'environnement (`~/.ai_env`)

**Utilisation:**
```bash
bash /opt/mcp/install.sh
```

**Post-installation:**
1. Ã‰diter `~/.ai_env` pour configurer vos clÃ©s API
2. Sourcer l'environnement: `source ~/.ai_env`
3. Tester les outils: `claude-test`, `ollama list`, `gemini-cli "test"`

---

### ğŸ‘ï¸ `watcher.sh` - Surveillance de fichiers

**RÃ´le:** Surveiller les rÃ©pertoires `/opt/mcp/` et `/data/` pour dÃ©tecter les changements de fichiers et dÃ©clencher des actions automatiques.

**FonctionnalitÃ©s:**
- Surveillance en temps rÃ©el avec `inotifywait`
- DÃ©tection des Ã©vÃ©nements: crÃ©ation, modification, suppression, dÃ©placement
- Actions personnalisables selon le type de fichier
- Logging dans `/tmp/mcp_watcher.log`

**Ã‰vÃ©nements surveillÃ©s:**
- `CREATE` - Nouveau fichier crÃ©Ã©
- `CLOSE_WRITE` - Fichier modifiÃ© et fermÃ©
- `DELETE` - Fichier supprimÃ©
- `MOVED_TO` / `MOVED_FROM` - Fichier dÃ©placÃ©

**Utilisation:**
```bash
# Lancement en avant-plan (mode debug)
bash /opt/mcp/watcher.sh

# Lancement en arriÃ¨re-plan
bash /opt/mcp/watcher.sh &

# ArrÃªter le watcher
pkill -f watcher.sh
```

**Extensions possibles (TODOs):**
- Analyse automatique de nouveaux fichiers avec Claude
- Indexation pour recherche full-text
- Synchronisation cloud (S3, Google Drive)
- Notifications webhook
- Tests automatiques sur modification de code
- Snapshots de `/data/`

---

### ğŸš€ `start.sh` - Point d'entrÃ©e principal

**RÃ´le:** Script de dÃ©marrage automatique appelÃ© au boot du conteneur. Initialise l'environnement et les services IA.

**Actions effectuÃ©es:**
1. Chargement des variables d'environnement (`~/.ai_env`)
2. VÃ©rification des rÃ©pertoires MCP et DATA
3. DÃ©marrage du serveur Ollama (si installÃ©)
4. Configuration du watcher (dÃ©sactivÃ© par dÃ©faut)
5. Affichage des instructions de dÃ©marrage

**Utilisation:**
```bash
bash /opt/mcp/start.sh
```

**AppelÃ© automatiquement:** Ce script est exÃ©cutÃ© par `/entrypoint.sh` au dÃ©marrage du conteneur.

**Extensions possibles (TODOs):**
- Healthcheck des services
- Lancement automatique d'agents au boot
- Configuration de tÃ¢ches cron
- Initialisation de bases de donnÃ©es
- DÃ©marrage d'un serveur web de contrÃ´le
- Synchronisation git au dÃ©marrage

---

### ğŸ¤– `start-agent.sh` - Lanceur d'agents IA

**RÃ´le:** Menu interactif pour lancer diffÃ©rents agents IA.

**Agents disponibles:**

#### 1ï¸âƒ£ **Claude CLI (Anthropic)**
- Mode CLI natif (si `claude` est installÃ©)
- Mode wrapper Python interactif (fallback)
- NÃ©cessite: `ANTHROPIC_API_KEY`

#### 2ï¸âƒ£ **Ollama (LLM local)**
- Serveur Ollama dÃ©marrÃ© automatiquement
- Liste des modÃ¨les disponibles
- Lancement interactif de modÃ¨les (llama2, mistral, etc.)

#### 3ï¸âƒ£ **Gemini CLI (Google)**
- Wrapper Python interactif
- NÃ©cessite: `GEMINI_API_KEY` ou `GOOGLE_API_KEY`

#### 4ï¸âƒ£ **Mode Python interactif**
- Shell Python avec tous les SDK IA prÃ©-importÃ©s
- `anthropic`, `openai`, `google-generativeai`

#### 5ï¸âƒ£ **Tous les services**
- DÃ©marrage de tous les services en arriÃ¨re-plan

**Utilisation:**
```bash
bash /opt/mcp/start-agent.sh
```

**Exemple de session:**
```bash
$ bash /opt/mcp/start-agent.sh

ğŸ¯ SÃ©lectionnez l'agent IA Ã  lancer:

  1) Claude CLI (Anthropic)
  2) Ollama (Local LLM)
  3) Gemini CLI (Google)
  4) Mode interactif Python
  5) Tous
  0) Quitter

Votre choix [1-5]: 1

ğŸ¤– Lancement de Claude CLI...
Vous > Bonjour Claude!
Claude > Bonjour! Comment puis-je vous aider aujourd'hui?
```

---

## ğŸ”‘ Configuration de l'environnement

### Fichier `~/.ai_env`

CrÃ©Ã© automatiquement par `install.sh`, ce fichier contient vos clÃ©s API:

```bash
# Anthropic Claude API
export ANTHROPIC_API_KEY="sk-ant-your-key-here"

# OpenAI API
export OPENAI_API_KEY="sk-your-key-here"

# Google Gemini API
export GEMINI_API_KEY="your-gemini-key-here"
export GOOGLE_API_KEY="your-google-key-here"

# RÃ©pertoires
export MCP_DIR="/opt/mcp"
export DATA_DIR="/data"
```

**Charger l'environnement:**
```bash
source ~/.ai_env
```

**Ajouter au `.bashrc` pour chargement automatique:**
```bash
echo "source ~/.ai_env" >> ~/.bashrc
```

---

## ğŸ› ï¸ Workflows recommandÃ©s

### Workflow 1: Installation initiale
```bash
# 1. Installer les outils
bash /opt/mcp/install.sh

# 2. Configurer les clÃ©s API
nano ~/.ai_env

# 3. Charger l'environnement
source ~/.ai_env

# 4. Tester
claude-test
ollama list
```

### Workflow 2: DÃ©veloppement avec surveillance
```bash
# Terminal 1: Lancer le watcher
bash /opt/mcp/watcher.sh

# Terminal 2: Travailler sur vos fichiers
cd /data/mon-projet
# Le watcher dÃ©tectera automatiquement les changements
```

### Workflow 3: Session IA interactive
```bash
# Lancer un agent
bash /opt/mcp/start-agent.sh
# SÃ©lectionner Claude CLI (1)
# Discuter avec Claude en mode interactif
```

---

## ğŸ“‚ Structure des rÃ©pertoires

```
/opt/mcp/          # Scripts MCP (ce rÃ©pertoire)
â”œâ”€â”€ install.sh     # Installation outils IA
â”œâ”€â”€ watcher.sh     # Surveillance fichiers
â”œâ”€â”€ start.sh       # DÃ©marrage services
â”œâ”€â”€ start-agent.sh # Lanceur agents IA
â””â”€â”€ README_mcp.md  # Cette documentation

/data/             # Volume persistant pour vos donnÃ©es
â””â”€â”€ (vos projets, fichiers, etc.)

/home/ia/          # Home de l'utilisateur IA
â”œâ”€â”€ .ai_env        # Variables d'environnement
â”œâ”€â”€ .bashrc        # Configuration bash
â”œâ”€â”€ .local/bin/    # Scripts personnels
â”‚   â”œâ”€â”€ claude-test
â”‚   â””â”€â”€ gemini-cli
â”œâ”€â”€ Bureau/        # Desktop XFCE
â”œâ”€â”€ Documents/     # Documents
â””â”€â”€ TÃ©lÃ©chargements/
```

---

## ğŸ”Œ IntÃ©gration avec les agents IA

### Utilisation de Claude depuis Python

```python
import os
from anthropic import Anthropic

# L'API key est dÃ©jÃ  dans l'environnement
client = Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])

message = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=2048,
    messages=[{
        "role": "user",
        "content": "Analyse le contenu de /data/projet/fichier.py"
    }]
)

print(message.content[0].text)
```

### Utilisation d'Ollama depuis Python

```python
import subprocess
import json

def query_ollama(prompt, model="llama2"):
    result = subprocess.run(
        ["ollama", "run", model, prompt],
        capture_output=True,
        text=True
    )
    return result.stdout

response = query_ollama("RÃ©sume ce texte: ...")
print(response)
```

### Utilisation de Gemini depuis Python

```python
import os
import google.generativeai as genai

genai.configure(api_key=os.environ["GEMINI_API_KEY"])
model = genai.GenerativeModel('gemini-pro')

response = model.generate_content("Explique le MCP")
print(response.text)
```

---

## ğŸš€ Extensions et personnalisations

### Ajouter un nouveau script MCP

```bash
# CrÃ©er votre script
nano /opt/mcp/mon-script.sh

# Le rendre exÃ©cutable
chmod +x /opt/mcp/mon-script.sh

# L'appeler depuis d'autres scripts
bash /opt/mcp/mon-script.sh
```

### Ajouter une action au watcher

Modifier `watcher.sh` dans la fonction `handle_file_event()`:

```bash
case "$event" in
    CREATE|CLOSE_WRITE)
        if [[ "$file" == *.md ]]; then
            # Action pour fichiers Markdown
            echo "Fichier Markdown dÃ©tectÃ©: $file"
            # Exemple: Convertir en PDF
            # pandoc "$file" -o "${file%.md}.pdf"
        fi
        ;;
esac
```

### CrÃ©er un service systemd (optionnel)

```bash
# CrÃ©er le fichier service
sudo nano /etc/systemd/system/mcp-watcher.service
```

Contenu:
```ini
[Unit]
Description=MCP File Watcher
After=network.target

[Service]
Type=simple
User=ia
WorkingDirectory=/opt/mcp
ExecStart=/bin/bash /opt/mcp/watcher.sh
Restart=on-failure

[Install]
WantedBy=multi-user.target
```

Activer:
```bash
sudo systemctl enable mcp-watcher.service
sudo systemctl start mcp-watcher.service
```

---

## ğŸ“ Logs et debugging

### Localisation des logs

- **Watcher**: `/tmp/mcp_watcher.log`
- **Ollama**: `/tmp/ollama.log`
- **Logs systÃ¨me**: `journalctl -u service-name`

### Consulter les logs

```bash
# Logs du watcher en temps rÃ©el
tail -f /tmp/mcp_watcher.log

# Logs d'Ollama
tail -f /tmp/ollama.log

# Tous les processus de l'utilisateur ia
ps aux | grep ia
```

---

## ğŸ“ Ressources supplÃ©mentaires

### Documentation officielle

- **Anthropic Claude**: https://docs.anthropic.com/
- **Ollama**: https://ollama.com/
- **Google Gemini**: https://ai.google.dev/
- **Model Context Protocol (MCP)**: https://modelcontextprotocol.io/

### Outils CLI

```bash
# VÃ©rifier les installations
which claude
which ollama
which python3

# Versions
ollama --version
python3 --version

# Liste des modÃ¨les Ollama
ollama list

# Pull d'un nouveau modÃ¨le
ollama pull mistral
ollama pull codellama
```

---

## âœ¨ Conclusion

Le rÃ©pertoire `/opt/mcp/` est le **centre de contrÃ´le** de votre environnement IA. Tous les scripts sont conÃ§us pour Ãªtre **modulaires, extensibles et personnalisables**.

N'hÃ©sitez pas Ã :
- Modifier les scripts selon vos besoins
- Ajouter vos propres scripts
- CrÃ©er des workflows automatisÃ©s
- IntÃ©grer de nouveaux outils IA

**Pour toute question, consultez le README principal du projet.**

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸš€ **Happy coding with AI!**
