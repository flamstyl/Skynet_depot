# ğŸŸ£ MCP_LM_Terminal - Local MCP Server for LM Studio + Terminal IA

**Serveur MCP local pour interfacer ChatGPT/Claude avec LM Studio et un terminal interactif**

---

## ğŸ“‹ Description

MCP_LM_Terminal est un serveur MCP (Model Context Protocol) conÃ§u pour :

âœ… **Interfacer ChatGPT/Claude avec LM Studio** via son API locale
âœ… **Ouvrir un terminal interactif** accessible Ã  l'IA
âœ… **Donner accÃ¨s Ã  des outils locaux** (fichiers, shell, logs)
âœ… **Servir de backend multi-agents** pour orchestrer IA + LM Studio
âœ… **Exporter les rÃ©ponses en temps rÃ©el** via WebSocket (optionnel)

---

## ğŸ¯ FonctionnalitÃ©s

### ğŸ”¹ Routes API MCP

| Endpoint | MÃ©thode | Description |
|----------|---------|-------------|
| `/status` | GET | Ã‰tat du serveur, modÃ¨le LM actif, info terminal |
| `/lm/query` | POST | Transmet une requÃªte Ã  LM Studio et retourne la rÃ©ponse **avec statistiques complÃ¨tes** |
| `/terminal/cmd` | POST | ExÃ©cute une commande shell et retourne stdout/stderr |
| `/terminal/stream` | WebSocket | Flux interactif bi-directionnel (terminal PTY) |

### ğŸ”¹ API LM Studio

- **Support API native v0** : `/api/v0/*` avec statistiques avancÃ©es
- **Support API OpenAI v1** : `/v1/*` compatible ChatGPT
- **Statistiques complÃ¨tes** : tokens/sec, TTFT, generation_time, model_info, runtime
- **ParamÃ¨tres avancÃ©s** : top_p, top_k, stop, ttl

### ğŸ”¹ SÃ©curitÃ©

- **Authentification via token API** (Header: `Authorization: Bearer <token>`)
- **Configuration centralisÃ©e** dans `config.json`
- **Timeout automatique** pour les commandes shell

### ğŸ”¹ Support Multi-Plateformes

- **Linux/Mac**: Terminal PTY complet (ptyprocess)
- **Windows**: Fallback subprocess automatique

---

## ğŸ“ Structure du Projet

```
MCP_LM_Terminal/
â”œâ”€â”€ server.py              # Serveur FastAPI principal
â”œâ”€â”€ terminal_handler.py    # Gestionnaire terminal PTY/subprocess
â”œâ”€â”€ lmstudio_client.py     # Client API LM Studio
â”œâ”€â”€ config.json            # Configuration (TOKEN, HOST, PORT)
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â””â”€â”€ README.md              # Documentation (ce fichier)
```

---

## ğŸš€ Installation

### 1ï¸âƒ£ PrÃ©requis

- **Python 3.11+** (recommandÃ©)
- **LM Studio** installÃ© et lancÃ© sur `http://localhost:1234`
- **Git** (optionnel)

### 2ï¸âƒ£ Installation des dÃ©pendances

```bash
# Cloner le dÃ©pÃ´t (ou tÃ©lÃ©charger les fichiers)
cd MCP_LM_Terminal

# CrÃ©er un environnement virtuel
python3 -m venv venv

# Activer l'environnement virtuel
# Linux/Mac:
source venv/bin/activate

# Windows:
venv\Scripts\activate

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configuration

**Modifier le fichier `config.json` :**

```json
{
  "api_token": "VOTRE_TOKEN_SECRET_ICI",
  "lmstudio": {
    "host": "http://localhost:1234",
    "model": "default",
    "api_version": "v0"
  },
  "terminal": {
    "timeout": 20
  },
  "server": {
    "host": "0.0.0.0",
    "port": 8080
  }
}
```

**ParamÃ¨tres de configuration :**
- `api_token` : **Changez obligatoirement ce token !**
- `api_version` : `"v0"` (API native avec stats) ou `"v1"` (API OpenAI-compatible)

---

## ğŸ® Utilisation

### ğŸ”¹ DÃ©marrage du Serveur

```bash
# S'assurer que l'environnement virtuel est activÃ©
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Lancer le serveur
uvicorn server:app --host 0.0.0.0 --port 8080
```

**Le serveur sera accessible sur** `http://localhost:8080`

### ğŸ”¹ VÃ©rification du Statut

```bash
curl -X GET http://localhost:8080/status \
  -H "Authorization: Bearer VOTRE_TOKEN_SECRET_ICI"
```

**RÃ©ponse :**

```json
{
  "status": "online",
  "lm_studio": {
    "connected": true,
    "host": "http://localhost:1234",
    "model": "default",
    "available": true
  },
  "terminal": {
    "status": "online",
    "timeout": "20s"
  },
  "version": "1.0.0"
}
```

### ğŸ”¹ Test Terminal

**ExÃ©cuter une commande shell :**

```bash
curl -X POST http://localhost:8080/terminal/cmd \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer VOTRE_TOKEN_SECRET_ICI" \
  -d '{"cmd": "ls -la"}'
```

**RÃ©ponse :**

```json
{
  "success": true,
  "command": "ls -la",
  "stdout": "total 48\ndrwxr-xr-x  6 user user 4096 ...",
  "stderr": "",
  "exit_code": 0,
  "execution_time": 0.12
}
```

### ğŸ”¹ Test LM Studio

**Envoyer une requÃªte au modÃ¨le LM :**

```bash
curl -X POST http://localhost:8080/lm/query \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer VOTRE_TOKEN_SECRET_ICI" \
  -d '{
    "prompt": "Bonjour, qui es-tu ?",
    "temperature": 0.7,
    "max_tokens": 512
  }'
```

**RÃ©ponse (avec statistiques complÃ¨tes) :**

```json
{
  "success": true,
  "content": "Je suis un assistant IA local exÃ©cutÃ© via LM Studio...",
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 48,
    "total_tokens": 63
  },
  "stats": {
    "tokens_per_second": 52.43,
    "time_to_first_token": 0.112,
    "generation_time": 0.915,
    "stop_reason": "eosFound"
  },
  "model_info": {
    "arch": "llama",
    "quant": "Q4_K_M",
    "format": "gguf",
    "context_length": 4096
  },
  "runtime": {
    "name": "llama.cpp-mac-arm64-apple-metal-advsimd",
    "version": "1.3.0",
    "supported_formats": ["gguf"]
  },
  "finish_reason": "stop",
  "model": "default"
}
```

**Note** : Les champs `stats`, `model_info` et `runtime` sont disponibles uniquement avec `api_version: "v0"` (API native LM Studio). Avec `api_version: "v1"`, seuls `content`, `usage` et `finish_reason` sont retournÃ©s.

---

## ğŸ”— Connexion avec ChatGPT/Claude

### ğŸ”¹ Avec ChatGPT (Custom Actions)

1. Aller dans **Settings > Actions**
2. CrÃ©er une nouvelle action **MCP_LM_Terminal**
3. **Schema OpenAPI** :

```yaml
openapi: 3.0.0
info:
  title: MCP LM Terminal
  version: 1.0.0
servers:
  - url: http://localhost:8080
paths:
  /lm/query:
    post:
      summary: Interroger LM Studio
      security:
        - BearerAuth: []
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                prompt:
                  type: string
                temperature:
                  type: number
                max_tokens:
                  type: integer
      responses:
        '200':
          description: RÃ©ponse du modÃ¨le
  /terminal/cmd:
    post:
      summary: ExÃ©cuter une commande
      security:
        - BearerAuth: []
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                cmd:
                  type: string
      responses:
        '200':
          description: RÃ©sultat de la commande
components:
  securitySchemes:
    BearerAuth:
      type: http
      scheme: bearer
```

4. **Authentication** : Bearer Token avec votre `api_token`

### ğŸ”¹ Avec Claude Desktop (MCP)

**Ajouter dans `claude_desktop_config.json` :**

```json
{
  "mcpServers": {
    "lm-terminal": {
      "command": "uvicorn",
      "args": [
        "server:app",
        "--host", "0.0.0.0",
        "--port", "8080"
      ],
      "cwd": "/chemin/vers/MCP_LM_Terminal",
      "env": {
        "PYTHONPATH": "/chemin/vers/MCP_LM_Terminal/venv/lib/python3.11/site-packages"
      }
    }
  }
}
```

**RedÃ©marrer Claude Desktop.**

---

## ğŸ§ª Tests Manuels

### ğŸ”¹ Test du Terminal Handler

```bash
python3 terminal_handler.py
```

**Sortie attendue :**

```
ğŸŸ£ Terminal Handler Info:
  system: Linux
  shell: /bin/bash
  pty_support: True
  platform: Linux-4.4.0-x86_64-with-glibc2.31

ğŸ§ª Test 1: ls -la
Exit code: 0
Output: total 48
drwxr-xr-x  6 user user ...
```

### ğŸ”¹ Test du Client LM Studio

```bash
python3 lmstudio_client.py
```

**Sortie attendue :**

```
ğŸ§ª Test LM Studio Client
==================================================

1ï¸âƒ£ VÃ©rification du statut...
ConnectÃ© : True
Disponible : True
ModÃ¨les : default, llama-3.2

2ï¸âƒ£ RÃ©cupÃ©ration des modÃ¨les...
ModÃ¨les trouvÃ©s : ['default', 'llama-3.2']

3ï¸âƒ£ Test de completion...
RÃ©ponse : Hello! How can I help you today?

âœ… Tests terminÃ©s
```

---

## ğŸ”§ Configuration AvancÃ©e

### ğŸ”¹ Changer le Port

Dans `config.json` :

```json
{
  "server": {
    "host": "0.0.0.0",
    "port": 9000
  }
}
```

### ğŸ”¹ Utiliser un ModÃ¨le SpÃ©cifique

Dans `config.json` :

```json
{
  "lmstudio": {
    "host": "http://localhost:1234",
    "model": "llama-3.2-3b-instruct"
  }
}
```

### ğŸ”¹ Augmenter le Timeout Terminal

Dans `config.json` :

```json
{
  "terminal": {
    "timeout": 60
  }
}
```

---

## ğŸ›¡ï¸ SÃ©curitÃ©

### ğŸ”¹ Token API

- **Ne partagez JAMAIS votre token**
- Utilisez un token complexe (minimum 32 caractÃ¨res)
- Exemple de gÃ©nÃ©ration :

```bash
python3 -c "import secrets; print(secrets.token_urlsafe(32))"
```

### ğŸ”¹ AccÃ¨s RÃ©seau

- Par dÃ©faut, le serveur Ã©coute sur `0.0.0.0` (toutes les interfaces)
- En production, restreignez Ã  `127.0.0.1` (localhost uniquement)

### ğŸ”¹ Commandes Terminal

- **Attention** : toutes les commandes sont exÃ©cutÃ©es avec les privilÃ¨ges de l'utilisateur
- **Ne jamais exposer ce serveur publiquement** sans authentification renforcÃ©e
- Utilisez des restrictions shell (chroot, docker, etc.) en production

---

## ğŸ› RÃ©solution de ProblÃ¨mes

### âŒ LM Studio non accessible

**Erreur :**
```
LM Studio non accessible (connexion refusÃ©e)
```

**Solution :**
1. VÃ©rifiez que LM Studio est lancÃ©
2. VÃ©rifiez qu'un modÃ¨le est chargÃ©
3. VÃ©rifiez que le serveur local est sur `http://localhost:1234`
4. Testez manuellement :
   ```bash
   curl http://localhost:1234/v1/models
   ```

### âŒ Erreur ptyprocess sur Windows

**Erreur :**
```
ModuleNotFoundError: No module named 'ptyprocess'
```

**Solution :**
- C'est normal sur Windows
- Le code utilise automatiquement `subprocess` en fallback
- Aucune action requise

### âŒ Token invalide

**Erreur :**
```
403 Forbidden: Token d'authentification invalide
```

**Solution :**
- VÃ©rifiez que le Header est bien : `Authorization: Bearer <token>`
- VÃ©rifiez que le token correspond au `config.json`
- Pas d'espaces ou caractÃ¨res spÃ©ciaux

---

## ğŸ“š Documentation API ComplÃ¨te

### GET /status

**Headers :**
```
Authorization: Bearer <token>
```

**RÃ©ponse :**
```json
{
  "status": "online",
  "lm_studio": {
    "connected": true,
    "host": "http://localhost:1234",
    "model": "default",
    "available": true
  },
  "terminal": {
    "status": "online",
    "timeout": "20s"
  },
  "version": "1.0.0"
}
```

### POST /lm/query

**Headers :**
```
Authorization: Bearer <token>
Content-Type: application/json
```

**Body :**
```json
{
  "prompt": "Votre question",
  "temperature": 0.7,
  "max_tokens": 512,
  "model": "default"
}
```

**RÃ©ponse (avec API v0) :**
```json
{
  "success": true,
  "content": "RÃ©ponse du modÃ¨le...",
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 50,
    "total_tokens": 60
  },
  "stats": {
    "tokens_per_second": 45.2,
    "time_to_first_token": 0.12,
    "generation_time": 1.1,
    "stop_reason": "eosFound"
  },
  "model_info": {
    "arch": "llama",
    "quant": "Q4_K_M",
    "format": "gguf",
    "context_length": 4096
  },
  "runtime": {
    "name": "llama.cpp-...",
    "version": "1.3.0",
    "supported_formats": ["gguf"]
  },
  "finish_reason": "stop",
  "model": "default"
}
```

### POST /terminal/cmd

**Headers :**
```
Authorization: Bearer <token>
Content-Type: application/json
```

**Body :**
```json
{
  "cmd": "ls -la",
  "timeout": 20
}
```

**RÃ©ponse :**
```json
{
  "success": true,
  "command": "ls -la",
  "stdout": "total 48\n...",
  "stderr": "",
  "exit_code": 0,
  "execution_time": 0.12
}
```

### WebSocket /terminal/stream

**Connexion :**
```javascript
const ws = new WebSocket('ws://localhost:8080/terminal/stream');

ws.onopen = () => {
  console.log('Terminal connectÃ©');
  ws.send('ls -la');
};

ws.onmessage = (event) => {
  const data = JSON.parse(event.data);
  console.log(data);
};
```

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues !

1. Fork le projet
2. CrÃ©ez une branche (`git checkout -b feature/ma-fonctionnalite`)
3. Commit vos changements (`git commit -m 'Ajout de ma fonctionnalitÃ©'`)
4. Push vers la branche (`git push origin feature/ma-fonctionnalite`)
5. Ouvrez une Pull Request

---

## ğŸ“„ Licence

Ce projet fait partie de **Skynet_depot** - Architecture multi-agents MCP

---

## ğŸ†˜ Support

Pour toute question ou problÃ¨me :

1. Consultez la section **RÃ©solution de ProblÃ¨mes**
2. VÃ©rifiez les logs du serveur
3. Ouvrez une issue sur GitHub

---

## ğŸ”® Roadmap

- [ ] Support du streaming LM Studio (rÃ©ponses progressives)
- [ ] Interface Web de monitoring
- [ ] Support multi-sessions terminal
- [ ] Logs persistants et rotation
- [ ] IntÃ©gration avec d'autres modÃ¨les locaux (Ollama, etc.)
- [ ] Mode daemon (systemd/supervisor)

---

**ğŸŸ£ MCP_LM_Terminal - Skynet Local Execution Unit**

*PropulsÃ© par FastAPI, LM Studio, et Claude Code 4.5*
